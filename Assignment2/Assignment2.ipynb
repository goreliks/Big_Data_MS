{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Alexander Gorelik\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of MapReduceEngine\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading a Jupyter notebook.\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "  \n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q1 - 5 points - Initial Steps\n",
    "- Q2 - 50 points - MapReduceEngine\n",
    "- Q3 - 30 points - Implement the MapReduce Inverted index of the JSON documents\n",
    "- Q4 - 5 points - Testing Your MapReduce\n",
    "- Q5 - 10 points - Final Thoughts \n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "!pip install --quiet zipfile36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import glob\n",
    "import threading # you can use easier threading packages\n",
    "import concurrent.futures\n",
    "import csv\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "secondname = ['Stroul', 'Gorelik', 'Snow', 'White', 'Ridley', 'Wahlberg', 'Hawking', 'Jordan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_index in range(1,21):\n",
    "    rows = []\n",
    "    for _ in range(10):\n",
    "        row = [f'{random.choice(firstname)}', f'{random.choice(secondname)}', f'{random.choice(city)}']\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows, columns=['firstname', 'secondname', 'city'])\n",
    "    df.to_csv(f'myCSV{csv_index}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreducetmp_dir = './mapreducetemp'\n",
    "mapreducefinal_dir = './mapreducefinal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [mapreducetmp_dir, mapreducefinal_dir]:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'tmp_database.db'\n",
    "\n",
    "CREATE_SCHEMA_STR = ''' CREATE TABLE IF NOT EXISTS temp_results (\n",
    "                                        key TEXT,\n",
    "                                        value TEXT\n",
    "                                    ); '''\n",
    "\n",
    "conn = None\n",
    "try:\n",
    "    conn = sqlite3.connect(database)\n",
    "    c = conn.cursor()\n",
    "    c.execute(CREATE_SCHEMA_STR)\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "\n",
    "class MapReduceEngine():\n",
    "        \n",
    "    def execute(self, input_data, map_function, reduce_function, params):\n",
    "        \n",
    "        workers_num = len(input_data)\n",
    "        \n",
    "        # using ThreadPoolExecutor as a context manager\n",
    "        # the end of the with block causes the ThreadPoolExecutor to do a .join() on each of the threads in the pool.\n",
    "        # map threads\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=workers_num) as executor:\n",
    "            for i, key in enumerate(input_data):\n",
    "                try:\n",
    "                    executor.submit(self.thread_map_helper, i, key, map_function, params)\n",
    "                except Exception as e:\n",
    "                    return e\n",
    "        \n",
    "        # at this point all threads completed there tasks\n",
    "        \n",
    "        # concatenating all csv files to a single df\n",
    "        tmp_dfs = []\n",
    "        for tmp_file in glob.glob(os.path.join(mapreducetmp_dir, '*.csv')):\n",
    "            data = pd.read_csv(tmp_file)\n",
    "            tmp_dfs.append(data)\n",
    "        tmp_dfs = pd.concat(tmp_dfs)\n",
    "        \n",
    "        # creating connection and qurying db\n",
    "        sql_conn = None\n",
    "        try:\n",
    "            sql_conn = sqlite3.connect(database)\n",
    "            tmp_dfs.to_sql('temp_results',sql_conn, if_exists='append',index=False)\n",
    "            \n",
    "            cur = sql_conn.cursor()\n",
    "            sort_and_shuffle_query = \"SELECT key, GROUP_CONCAT(value) as value \" \\\n",
    "                                     \"FROM temp_results \" \\\n",
    "                                     \"GROUP BY key \" \\\n",
    "                                     \"ORDER BY key \"\n",
    "            cur.execute(sort_and_shuffle_query)\n",
    "            sort_and_shuffle_res = cur.fetchall()\n",
    "        except Error as e:\n",
    "            return e\n",
    "        finally:\n",
    "            if sql_conn:\n",
    "                sql_conn.close()\n",
    "        \n",
    "        # reduce threads\n",
    "        workers_num = len(sort_and_shuffle_res)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=workers_num) as executor:\n",
    "            for i, res in enumerate(sort_and_shuffle_res):\n",
    "                try:\n",
    "                    executor.submit(self.thread_reduce_helper, i, res, reduce_function)\n",
    "                except Exception as e:\n",
    "                    return e\n",
    "                \n",
    "        return 'MapReduce Completed'\n",
    "                        \n",
    "        \n",
    "                \n",
    "    def thread_map_helper(self, i, key, map_function, params):\n",
    "        # every thread runs map_function, getting result and saves it to the csv file\n",
    "        result = map_function(key, params['column'])\n",
    "        df = pd.DataFrame(result, columns =['key', 'value'])\n",
    "        df.to_csv(f'mapreducetemp/part-tmp-{i}.csv', index=False)\n",
    "        \n",
    "    def thread_reduce_helper(self, i, res, reduce_function):\n",
    "        # every thread runs reduce_function, getting result and saves it to the csv file\n",
    "        result = reduce_function(res)\n",
    "        df = pd.DataFrame({'key': result[0], 'value': result[1]}, index=[0])\n",
    "        df.to_csv(f'mapreducefinal/part-{i}-final.csv', index=False)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name, column_index=0):\n",
    "    \n",
    "    csv_df = pd.read_csv(document_name)\n",
    "    # extracting keys from csv to list\n",
    "    keys = csv_df.iloc[:,column_index].tolist()\n",
    "    # returning list of tuples (key_value, document_name)\n",
    "    return list(zip(keys, [document_name]*len(keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'myCSV4.csv'),\n",
       " ('Michael', 'myCSV4.csv'),\n",
       " ('Albert', 'myCSV4.csv'),\n",
       " ('Dana', 'myCSV4.csv'),\n",
       " ('Michael', 'myCSV4.csv'),\n",
       " ('John', 'myCSV4.csv'),\n",
       " ('Michael', 'myCSV4.csv'),\n",
       " ('Albert', 'myCSV4.csv'),\n",
       " ('Michael', 'myCSV4.csv'),\n",
       " ('John', 'myCSV4.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_map('myCSV4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(documents):\n",
    "\n",
    "    # extracting key and value\n",
    "    res_list = [documents[0]]    \n",
    "    value = documents[1]\n",
    "    \n",
    "    # split value by ',' to get list of documents names and remove duplicates using set\n",
    "    docs_set = set(value.split(','))\n",
    "    # creating new \"value\" string from the set with no duplicates\n",
    "    docs_str = ','.join(docs_set)\n",
    "    \n",
    "    #appending value to res list \n",
    "    res_list.append(docs_str)\n",
    "    \n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = glob.glob('myCSV*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['myCSV1.csv',\n",
       " 'myCSV2.csv',\n",
       " 'myCSV3.csv',\n",
       " 'myCSV7.csv',\n",
       " 'myCSV6.csv',\n",
       " 'myCSV4.csv',\n",
       " 'myCSV5.csv',\n",
       " 'myCSV19.csv',\n",
       " 'myCSV18.csv',\n",
       " 'myCSV20.csv',\n",
       " 'myCSV10.csv',\n",
       " 'myCSV11.csv',\n",
       " 'myCSV13.csv',\n",
       " 'myCSV12.csv',\n",
       " 'myCSV16.csv',\n",
       " 'myCSV17.csv',\n",
       " 'myCSV15.csv',\n",
       " 'myCSV14.csv',\n",
       " 'myCSV8.csv',\n",
       " 'myCSV9.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce Completed\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, params={'column':0})\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('./mapreducetemp/*.csv'):\n",
    "    os.remove(file)\n",
    "    \n",
    "os.remove(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of transferring data via Network from mapper to reducer is Shuffling. Actually the output of Shuffle process serves as input to reduce tasks, otherwise, the reducers will not have any input or as mentioned in the question, reducers will need to read responses from the mappers directly.\n",
    "In this case, reducers will not get all the values for a specific key from all the mappers, for example if we look on the count words example from the class, the output of 4 mappers were:\n",
    "\n",
    "Apple 1 <br> \n",
    "Orange 1 <br>\n",
    "Mango 1 <br>\n",
    "\n",
    "----\n",
    "\n",
    "Orange 1 <br>\n",
    "Grapes 1 <br>\n",
    "Plum 1 <br>\n",
    "\n",
    "----\n",
    "\n",
    "Apple 1 <br>\n",
    "Plum 1 <br>\n",
    "Mango 1 <br>\n",
    "\n",
    "----\n",
    "\n",
    "Apple 1 <br>\n",
    "Apple 1 <br>\n",
    "Plum 1 <br>\n",
    "\n",
    "---\n",
    "\n",
    "If we have Shuffle phase we will be able transfer each key/value (having the same key) reducer so it will be able to reduce by key by one reducer. Since we will have:\n",
    "\n",
    "Apple 1 <br>\n",
    "Apple 1 <br>\n",
    "Apple 1 <br>\n",
    "Apple 1 <br>\n",
    "\n",
    "---\n",
    "\n",
    "Grapes 1 <br>\n",
    "\n",
    "---\n",
    "\n",
    "Mango 1 <br>\n",
    "Mango 1 <br>\n",
    "\n",
    "---\n",
    "\n",
    "Orange 1 <br>\n",
    "Orange 1 <br>\n",
    "\n",
    "---\n",
    "\n",
    "Plum 1 <br>\n",
    "Plum 1 <br>\n",
    "Plum 1 <br>\n",
    "\n",
    "we will run reduce function 5 times.\n",
    "\n",
    "<br>\n",
    "But in case reducers will read from mappers directly, we will need to run reducer per key on mappers output, meaning we end up with running reducers on the same key on different machines, so as a result we will run reduce function more times, in our example, 11 times, as:\n",
    "\n",
    "Output of 1st mapper has 3 unique keys\n",
    "\n",
    "Output of 2nd mapper has 3 unique keys\n",
    "\n",
    "Output of 3d mapper has 3 unique keys\n",
    "\n",
    "Output of 4th mapper has 2 unique keys\n",
    "\n",
    "---\n",
    "\n",
    "So when we are talking about Big Data we will end up with much more and not acceptable processing time and I think this is the main problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
