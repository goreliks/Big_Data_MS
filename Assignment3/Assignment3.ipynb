{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 3: ServerLess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "John Doe, 300123123  \n",
    "Jane Doe, 200123123\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of Serverless\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading (not Zip):\n",
    "    - Jupyter Notebook\n",
    "    - 2 Log files\n",
    "    - Additional local scripts\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "\n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q0 - 10 points - Setup\n",
    "- Q1 - 40 points - Serverless MapReduceEngine\n",
    "- Q2 - 20 points - MapReduce job to calculate inverted index\n",
    "- Q3 - 30 points - Shuffle\n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 0\n",
    "## Setup\n",
    "\n",
    "1. Navigate to IBM Cloud and open a trial account. No need to provide a credit card\n",
    "2. Choose IBM Cloud Object Storage service from the catalog\n",
    "3. Create a new bucket in IBM Cloud Object Storage\n",
    "4. Create credentials for the bucket with HMAC (access key and secret key)\n",
    "5. Choose IBM Cloud Functions service from the catalog and create a service\n",
    "\n",
    "\n",
    "#### Lithops setup\n",
    "1. By using “git” tool, install master branch of the Lithops project from\n",
    "https://github.com/lithops-cloud/lithops\n",
    "2. Follow Lithops documentation and configure Lithops against IBM Cloud Functions and IBM Cloud Object Storage\n",
    "3. Configure Lithops log level to be in DEBUG mode\n",
    "4. Run Hello World example by using Futures API and verify all is working properly.\n",
    "\n",
    "\n",
    "#### IBM Cloud Object Storage setup\n",
    "1. Upload all the input CSV files that you used in homework 2 into the bucket you created in IBM Cloud Object Storage\n",
    "\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## Serverless MapReduceEngine\n",
    "\n",
    "Modify MapReduceEngine from homework 2 into the MapReduceServerlessEngine where map and reduce tasks executed as a serverless actions, instead of local threads. In particular:\n",
    "1. Deploy all map tasks as a serverless actions by using Lithops against IBM Cloud Functions.\n",
    "2. Collect results from all map tasks and store them in the same SQLite as you used in MapReduceEngine and use the same code for the sort and shuffle phase.\n",
    "3. Deploy reduce tasks by using Lithops against IBM Cloud Functions. Instead of persisting results from reduce tasks, return results back to the MapReduceServerlessEngine and proceed with the same workflow as in MapReduceEngine\n",
    "4. Return results of reduce tasks to the user\n",
    "\n",
    "**Please attach:**  \n",
    "Text file with all log messages Lithops printed to console during the execution. Make\n",
    "sure log level is set to DEBUG mode.\n",
    "\n",
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lithops import FunctionExecutor\n",
    "from lithops import Storage\n",
    "\n",
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import glob\n",
    "\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uploading all myCSV files from Assignment 2 to IBM Cloud Object Storage using lithops Storage API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for my_csv in glob.glob('../Assignment2/myCSV*.csv'):\n",
    "    with open(f'{my_csv}', 'rb') as fl:\n",
    "        st.put_object('idc.g.test', f'{my_csv}', fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating SQLite database:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'tmp_database.db'\n",
    "\n",
    "CREATE_SCHEMA_STR = ''' CREATE TABLE IF NOT EXISTS temp_results (\n",
    "                                        key TEXT,\n",
    "                                        value TEXT\n",
    "                                    ); '''\n",
    "\n",
    "conn = None\n",
    "try:\n",
    "    conn = sqlite3.connect(database)\n",
    "    c = conn.cursor()\n",
    "    c.execute(CREATE_SCHEMA_STR)\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MapReduceServerlessEngine:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapReduceServerlessEngine():\n",
    "        \n",
    "    def execute(self, input_data, map_function, reduce_function, params):\n",
    "        \n",
    "        # using FunctionExecutor as a context manager\n",
    "        # inside using map function that spawn multiple function activations based on the items of an input list.\n",
    "        with FunctionExecutor() as fexec:\n",
    "            fut = fexec.map(map_function, input_data[:3], extra_args=[params])\n",
    "            #print(fut.get_result())\n",
    "            results = fut.get_result()\n",
    "        \n",
    "        # at this point all maps completed and their results returned in a list (list of lists)\n",
    "        \n",
    "        # flatten the list of lists to one list\n",
    "        flat_results = [item for sublist in results for item in sublist]\n",
    "        \n",
    "        # creating dataframe from all results\n",
    "        tmp_dfs = pd.DataFrame(flat_results, columns=['key', 'value'])\n",
    "        \n",
    "        # creating connection and qurying db\n",
    "        sql_conn = None\n",
    "        try:\n",
    "            sql_conn = sqlite3.connect(database)\n",
    "            tmp_dfs.to_sql('temp_results',sql_conn, if_exists='append',index=False)\n",
    "            \n",
    "            cur = sql_conn.cursor()\n",
    "            sort_and_shuffle_query = \"SELECT key, GROUP_CONCAT(value) as value \" \\\n",
    "                                     \"FROM temp_results \" \\\n",
    "                                     \"GROUP BY key \" \\\n",
    "                                     \"ORDER BY key \"\n",
    "            cur.execute(sort_and_shuffle_query)\n",
    "            sort_and_shuffle_res = cur.fetchall()\n",
    "        except Error as e:\n",
    "            return e\n",
    "        finally:\n",
    "            if sql_conn:\n",
    "                sql_conn.close()\n",
    "                \n",
    "        list_of_results = [list(res) for res in sort_and_shuffle_res]\n",
    "        \n",
    "        # reduce part        \n",
    "        with FunctionExecutor() as fexec:\n",
    "            fut = fexec.map(reduce_function, list_of_results)\n",
    "            reduce_results = fut.get_result()\n",
    "                \n",
    "        print('MapReduce Completed')\n",
    "        return reduce_results           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Submit MapReduce job to calculate inverted index\n",
    "1. Use input_data: `cos://bucket/<path to CSV data>`\n",
    "2. Submit MapReduce job with reduce and map functions as you used in homework 2, as follows\n",
    "\n",
    "    `mapreduce = MapReduceServerlessEngine()`  \n",
    "    `results = mapreduce.execute(input_data, inverted_map, inverted_index)`   \n",
    "    `print(results)`\n",
    "\n",
    "**Please attach:**  \n",
    "Text file with all log messages Lithops printed to console during the execution. Make\n",
    "sure log level is set to DEBUG mode.\n",
    "\n",
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 19:20:13,321 [DEBUG] lithops.config -- Loading configuration from /Users/gorelik/PycharmProjects/BigData/MS_Big_Data/Assignment3/.lithops_config\n",
      "2022-01-04 19:20:13,335 [DEBUG] lithops.config -- Loading Storage backend module: ibm_cos\n",
      "2022-01-04 19:20:13,340 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Creating IBM COS client\n",
      "2022-01-04 19:20:13,341 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Set IBM COS Endpoint to https://s3.eu-de.cloud-object-storage.appdomain.cloud\n",
      "2022-01-04 19:20:13,341 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Using access_key and secret_key\n",
      "2022-01-04 19:20:13,346 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n"
     ]
    }
   ],
   "source": [
    "# pulling all csv file names from my bucket in object storage\n",
    "keys_list = st.list_keys('idc.g.test', prefix='myCSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.put_object('idc.g.test', 'test_upload_myCSV1.csv', 'Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cos://idc.g.test/myCSV1.csv',\n",
       " 'cos://idc.g.test/myCSV10.csv',\n",
       " 'cos://idc.g.test/myCSV11.csv',\n",
       " 'cos://idc.g.test/myCSV12.csv',\n",
       " 'cos://idc.g.test/myCSV13.csv',\n",
       " 'cos://idc.g.test/myCSV14.csv',\n",
       " 'cos://idc.g.test/myCSV15.csv',\n",
       " 'cos://idc.g.test/myCSV16.csv',\n",
       " 'cos://idc.g.test/myCSV17.csv',\n",
       " 'cos://idc.g.test/myCSV18.csv',\n",
       " 'cos://idc.g.test/myCSV19.csv',\n",
       " 'cos://idc.g.test/myCSV2.csv',\n",
       " 'cos://idc.g.test/myCSV20.csv',\n",
       " 'cos://idc.g.test/myCSV3.csv',\n",
       " 'cos://idc.g.test/myCSV4.csv',\n",
       " 'cos://idc.g.test/myCSV5.csv',\n",
       " 'cos://idc.g.test/myCSV6.csv',\n",
       " 'cos://idc.g.test/myCSV7.csv',\n",
       " 'cos://idc.g.test/myCSV8.csv',\n",
       " 'cos://idc.g.test/myCSV9.csv']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding location for further processing\n",
    "input_data = ['cos://idc.g.test/' + key for key in keys_list]\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 21:58:19,992 [INFO] lithops.config -- Lithops v2.5.8\n",
      "2022-01-04 21:58:20,002 [DEBUG] lithops.config -- Loading configuration from /Users/gorelik/PycharmProjects/BigData/MS_Big_Data/Assignment3/.lithops_config\n",
      "2022-01-04 21:58:20,006 [DEBUG] lithops.config -- Loading Serverless backend module: ibm_cf\n",
      "2022-01-04 21:58:20,007 [DEBUG] lithops.config -- Loading Storage backend module: ibm_cos\n",
      "2022-01-04 21:58:20,008 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Creating IBM COS client\n",
      "2022-01-04 21:58:20,008 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Set IBM COS Endpoint to https://s3.eu-de.cloud-object-storage.appdomain.cloud\n",
      "2022-01-04 21:58:20,008 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Using access_key and secret_key\n",
      "2022-01-04 21:58:20,013 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n",
      "2022-01-04 21:58:20,014 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Creating IBM Cloud Functions client\n",
      "2022-01-04 21:58:20,014 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Set IBM CF Namespace to gorelik@me.com_dev\n",
      "2022-01-04 21:58:20,014 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Set IBM CF Endpoint to https://eu-gb.functions.cloud.ibm.com\n",
      "2022-01-04 21:58:20,015 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: eu-gb - Namespace: gorelik@me.com_dev\n",
      "2022-01-04 21:58:20,016 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 - Invoker initialized. Max workers: 1200\n",
      "2022-01-04 21:58:20,016 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 - Serverless invoker created\n",
      "2022-01-04 21:58:20,016 [DEBUG] lithops.executors -- Function executor for ibm_cf created with ID: ee8043-70\n",
      "2022-01-04 21:58:20,017 [INFO] lithops.invokers -- ExecutorID ee8043-70 | JobID M000 - Selected Runtime: lithopscloud/ibmcf-python-v38 - 256MB\n",
      "2022-01-04 21:58:20,017 [DEBUG] lithops.storage.storage -- Runtime metadata found in local memory cache\n",
      "2022-01-04 21:58:20,017 [DEBUG] lithops.job.job -- ExecutorID ee8043-70 | JobID M000 - Calling map on partitions from object storage flow\n",
      "2022-01-04 21:58:20,018 [DEBUG] lithops.job.partitioner -- Parsing input data\n",
      "2022-01-04 21:58:20,018 [DEBUG] lithops.job.partitioner -- Chunk size and chunk number not set \n",
      "2022-01-04 21:58:20,019 [DEBUG] lithops.job.partitioner -- Listing objects in 'ibm_cos://idc.g.test/'\n",
      "2022-01-04 21:58:20,709 [DEBUG] lithops.job.partitioner -- Total objects found: 90\n",
      "2022-01-04 21:58:20,710 [DEBUG] lithops.job.partitioner -- Creating 1 partitions from object myCSV1.csv (231.0B)\n",
      "2022-01-04 21:58:20,710 [DEBUG] lithops.job.partitioner -- Creating 1 partitions from object myCSV10.csv (227.0B)\n",
      "2022-01-04 21:58:20,711 [DEBUG] lithops.job.partitioner -- Creating 1 partitions from object myCSV11.csv (224.0B)\n",
      "2022-01-04 21:58:20,711 [DEBUG] lithops.job.job -- ExecutorID ee8043-70 | JobID M000 - Serializing function and data\n",
      "2022-01-04 21:58:20,713 [DEBUG] lithops.job.serialize -- Referenced modules: /Users/gorelik/opt/anaconda3/envs/big_data/lib/python3.8/site-packages/lithops/storage/utils.py\n",
      "2022-01-04 21:58:20,714 [DEBUG] lithops.job.serialize -- Modules to transmit: None\n",
      "2022-01-04 21:58:20,715 [DEBUG] lithops.job.job -- ExecutorID ee8043-70 | JobID M000 - Uploading function and modules to the storage backend\n",
      "2022-01-04 21:58:20,958 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- PUT Object lithops.jobs/ee8043-70/46c9dcfefad2e60e0dd9ccaea31f7cc9.func.pickle - Size: 817.0B - OK\n",
      "2022-01-04 21:58:20,966 [DEBUG] lithops.job.job -- ExecutorID ee8043-70 | JobID M000 - Data per activation is < 8.0KiB. Passing data through invocation payload\n",
      "2022-01-04 21:58:20,967 [INFO] lithops.invokers -- ExecutorID ee8043-70 | JobID M000 - Starting function invocation: inverted_map() - Total: 3 activations\n",
      "2022-01-04 21:58:20,967 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 | JobID M000 - Worker processes: 1 - Chunksize: 1\n",
      "2022-01-04 21:58:20,971 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 - Async invoker 0 started\n",
      "2022-01-04 21:58:20,979 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 - Async invoker 1 started\n",
      "2022-01-04 21:58:20,979 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 | JobID M000 - Free workers: 1200 - Going to run 3 activations in 3 workers\n",
      "2022-01-04 21:58:20,984 [INFO] lithops.invokers -- ExecutorID ee8043-70 | JobID M000 - View execution logs at /private/var/folders/jg/8775l8r94lj_rxbjb0d_th_h0000gn/T/lithops/logs/ee8043-70-M000.log\n",
      "2022-01-04 21:58:20,986 [DEBUG] lithops.monitor -- ExecutorID ee8043-70 - Starting Storage job monitor\n",
      "2022-01-04 21:58:20,986 [INFO] lithops.wait -- ExecutorID ee8043-70 - Getting results from functions\n",
      "2022-01-04 21:58:21,219 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 | JobID M000 - Calls 00002 invoked (0.234s) - Activation ID: 9407434ab96340f287434ab96330f2ba\n",
      "2022-01-04 21:58:21,230 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 | JobID M000 - Calls 00000 invoked (0.248s) - Activation ID: 3dfbe3664b4f4e7ebbe3664b4f5e7e2f\n",
      "2022-01-04 21:58:21,234 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 | JobID M000 - Calls 00001 invoked (0.250s) - Activation ID: 615f3db780f24c529f3db780f23c5217\n",
      "2022-01-04 21:58:23,631 [DEBUG] lithops.monitor -- ExecutorID ee8043-70 - Pending: 0 - Running: 0 - Done: 3\n",
      "2022-01-04 21:58:23,633 [DEBUG] lithops.monitor -- ExecutorID ee8043-70 - Storage job monitor finished\n",
      "2022-01-04 21:58:24,014 [DEBUG] lithops.future -- ExecutorID ee8043-70 | JobID M000 - Got status from call 00000 - Activation ID: 3dfbe3664b4f4e7ebbe3664b4f5e7e2f - Time: 0.34 seconds\n",
      "2022-01-04 21:58:24,016 [DEBUG] lithops.future -- ExecutorID ee8043-70 | JobID M000 - Got status from call 00002 - Activation ID: 9407434ab96340f287434ab96330f2ba - Time: 0.34 seconds\n",
      "2022-01-04 21:58:24,018 [DEBUG] lithops.future -- ExecutorID ee8043-70 | JobID M000 - Got status from call 00001 - Activation ID: 615f3db780f24c529f3db780f23c5217 - Time: 0.31 seconds\n",
      "2022-01-04 21:58:24,123 [DEBUG] lithops.future -- ExecutorID ee8043-70 | JobID M000 - Got output from call 00000 - Activation ID: 3dfbe3664b4f4e7ebbe3664b4f5e7e2f\n",
      "2022-01-04 21:58:24,134 [DEBUG] lithops.future -- ExecutorID ee8043-70 | JobID M000 - Got output from call 00001 - Activation ID: 615f3db780f24c529f3db780f23c5217\n",
      "2022-01-04 21:58:24,136 [DEBUG] lithops.future -- ExecutorID ee8043-70 | JobID M000 - Got output from call 00002 - Activation ID: 9407434ab96340f287434ab96330f2ba\n",
      "2022-01-04 21:58:24,141 [INFO] lithops.executors -- ExecutorID ee8043-70 - Cleaning temporary data\n",
      "2022-01-04 21:58:24,179 [DEBUG] lithops.executors -- ExecutorID ee8043-70 - Finished getting results\n",
      "2022-01-04 21:58:24,182 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 - Stopping async invokers\n",
      "2022-01-04 21:58:24,190 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 - Async invoker 0 finished\n",
      "2022-01-04 21:58:24,190 [DEBUG] lithops.invokers -- ExecutorID ee8043-70 - Async invoker 1 finished\n",
      "2022-01-04 21:58:24,195 [INFO] lithops.config -- Lithops v2.5.8\n",
      "2022-01-04 21:58:24,196 [DEBUG] lithops.config -- Loading configuration from /Users/gorelik/PycharmProjects/BigData/MS_Big_Data/Assignment3/.lithops_config\n",
      "2022-01-04 21:58:24,198 [DEBUG] lithops.config -- Loading Serverless backend module: ibm_cf\n",
      "2022-01-04 21:58:24,199 [DEBUG] lithops.config -- Loading Storage backend module: ibm_cos\n",
      "2022-01-04 21:58:24,200 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Creating IBM COS client\n",
      "2022-01-04 21:58:24,200 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Set IBM COS Endpoint to https://s3.eu-de.cloud-object-storage.appdomain.cloud\n",
      "2022-01-04 21:58:24,200 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Using access_key and secret_key\n",
      "2022-01-04 21:58:24,206 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n",
      "2022-01-04 21:58:24,206 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Creating IBM Cloud Functions client\n",
      "2022-01-04 21:58:24,207 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Set IBM CF Namespace to gorelik@me.com_dev\n",
      "2022-01-04 21:58:24,207 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Set IBM CF Endpoint to https://eu-gb.functions.cloud.ibm.com\n",
      "2022-01-04 21:58:24,207 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: eu-gb - Namespace: gorelik@me.com_dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 21:58:24,208 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 - Invoker initialized. Max workers: 1200\n",
      "2022-01-04 21:58:24,208 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 - Serverless invoker created\n",
      "2022-01-04 21:58:24,209 [DEBUG] lithops.executors -- Function executor for ibm_cf created with ID: ee8043-71\n",
      "2022-01-04 21:58:24,209 [INFO] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Selected Runtime: lithopscloud/ibmcf-python-v38 - 256MB\n",
      "2022-01-04 21:58:24,210 [DEBUG] lithops.storage.storage -- Runtime metadata found in local memory cache\n",
      "2022-01-04 21:58:24,210 [DEBUG] lithops.job.job -- ExecutorID ee8043-71 | JobID M000 - Serializing function and data\n",
      "2022-01-04 21:58:24,211 [DEBUG] lithops.job.serialize -- Referenced modules: None\n",
      "2022-01-04 21:58:24,212 [DEBUG] lithops.job.serialize -- Modules to transmit: None\n",
      "2022-01-04 21:58:24,212 [DEBUG] lithops.job.job -- ExecutorID ee8043-71 | JobID M000 - Uploading function and modules to the storage backend\n",
      "2022-01-04 21:58:24,688 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- PUT Object lithops.jobs/ee8043-71/03bee660b4d1fc0d8902c5143309dc6b.func.pickle - Size: 705.0B - OK\n",
      "2022-01-04 21:58:24,690 [DEBUG] lithops.job.job -- ExecutorID ee8043-71 | JobID M000 - Data per activation is < 8.0KiB. Passing data through invocation payload\n",
      "2022-01-04 21:58:24,691 [INFO] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Starting function invocation: inverted_reduce() - Total: 8 activations\n",
      "2022-01-04 21:58:24,691 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Worker processes: 1 - Chunksize: 1\n",
      "2022-01-04 21:58:24,695 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 - Async invoker 0 started\n",
      "2022-01-04 21:58:24,697 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 - Async invoker 1 started\n",
      "2022-01-04 21:58:24,697 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Free workers: 1200 - Going to run 8 activations in 8 workers\n",
      "2022-01-04 21:58:24,709 [INFO] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - View execution logs at /private/var/folders/jg/8775l8r94lj_rxbjb0d_th_h0000gn/T/lithops/logs/ee8043-71-M000.log\n",
      "2022-01-04 21:58:24,710 [DEBUG] lithops.monitor -- ExecutorID ee8043-71 - Starting Storage job monitor\n",
      "2022-01-04 21:58:24,711 [INFO] lithops.wait -- ExecutorID ee8043-71 - Getting results from functions\n",
      "2022-01-04 21:58:24,936 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Calls 00007 invoked (0.228s) - Activation ID: 704b5fada1954b448b5fada195ab44d6\n",
      "2022-01-04 21:58:24,946 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Calls 00000 invoked (0.246s) - Activation ID: d020f29a8a8b4f9da0f29a8a8bbf9de8\n",
      "2022-01-04 21:58:24,949 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Calls 00003 invoked (0.246s) - Activation ID: e452d4e4faae43be92d4e4faae53be56\n",
      "2022-01-04 21:58:24,951 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Calls 00004 invoked (0.246s) - Activation ID: 07eb8d39f776476fab8d39f776b76f94\n",
      "2022-01-04 21:58:24,954 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Calls 00006 invoked (0.247s) - Activation ID: 8aad515351e5489ead515351e5489e28\n",
      "2022-01-04 21:58:24,955 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Calls 00001 invoked (0.254s) - Activation ID: 17ecd4121a124028acd4121a12d0286b\n",
      "2022-01-04 21:58:24,957 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Calls 00002 invoked (0.255s) - Activation ID: 386465781c204f1ca465781c207f1ca7\n",
      "2022-01-04 21:58:24,958 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 | JobID M000 - Calls 00005 invoked (0.252s) - Activation ID: 5f53857021a1448b93857021a1348bcf\n",
      "2022-01-04 21:58:27,426 [DEBUG] lithops.monitor -- ExecutorID ee8043-71 - Pending: 0 - Running: 0 - Done: 8\n",
      "2022-01-04 21:58:27,430 [DEBUG] lithops.monitor -- ExecutorID ee8043-71 - Storage job monitor finished\n",
      "2022-01-04 21:58:27,739 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got status from call 00000 - Activation ID: d020f29a8a8b4f9da0f29a8a8bbf9de8 - Time: 0.20 seconds\n",
      "2022-01-04 21:58:27,756 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got status from call 00001 - Activation ID: 17ecd4121a124028acd4121a12d0286b - Time: 0.20 seconds\n",
      "2022-01-04 21:58:27,759 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got status from call 00002 - Activation ID: 386465781c204f1ca465781c207f1ca7 - Time: 0.23 seconds\n",
      "2022-01-04 21:58:27,763 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got status from call 00003 - Activation ID: e452d4e4faae43be92d4e4faae53be56 - Time: 0.24 seconds\n",
      "2022-01-04 21:58:27,763 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got status from call 00005 - Activation ID: 5f53857021a1448b93857021a1348bcf - Time: 0.23 seconds\n",
      "2022-01-04 21:58:27,766 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got status from call 00004 - Activation ID: 07eb8d39f776476fab8d39f776b76f94 - Time: 0.20 seconds\n",
      "2022-01-04 21:58:27,770 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got status from call 00006 - Activation ID: 8aad515351e5489ead515351e5489e28 - Time: 0.24 seconds\n",
      "2022-01-04 21:58:27,773 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got status from call 00007 - Activation ID: 704b5fada1954b448b5fada195ab44d6 - Time: 0.29 seconds\n",
      "2022-01-04 21:58:27,845 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got output from call 00000 - Activation ID: d020f29a8a8b4f9da0f29a8a8bbf9de8\n",
      "2022-01-04 21:58:27,857 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got output from call 00003 - Activation ID: e452d4e4faae43be92d4e4faae53be56\n",
      "2022-01-04 21:58:27,860 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got output from call 00002 - Activation ID: 386465781c204f1ca465781c207f1ca7\n",
      "2022-01-04 21:58:27,865 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got output from call 00001 - Activation ID: 17ecd4121a124028acd4121a12d0286b\n",
      "2022-01-04 21:58:27,866 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got output from call 00005 - Activation ID: 5f53857021a1448b93857021a1348bcf\n",
      "2022-01-04 21:58:27,867 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got output from call 00004 - Activation ID: 07eb8d39f776476fab8d39f776b76f94\n",
      "2022-01-04 21:58:27,870 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got output from call 00006 - Activation ID: 8aad515351e5489ead515351e5489e28\n",
      "2022-01-04 21:58:27,871 [DEBUG] lithops.future -- ExecutorID ee8043-71 | JobID M000 - Got output from call 00007 - Activation ID: 704b5fada1954b448b5fada195ab44d6\n",
      "2022-01-04 21:58:27,874 [INFO] lithops.executors -- ExecutorID ee8043-71 - Cleaning temporary data\n",
      "2022-01-04 21:58:27,875 [DEBUG] lithops.executors -- ExecutorID ee8043-71 - Finished getting results\n",
      "2022-01-04 21:58:27,875 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 - Stopping async invokers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Albert', 'myCSV11.csv,myCSV1.csv,myCSV10.csv'], ['Dana', 'myCSV1.csv,myCSV10.csv,myCSV11.csv'], ['Johanna', 'myCSV1.csv,myCSV11.csv,myCSV10.csv'], ['John', 'myCSV10.csv,myCSV1.csv'], ['Marc', 'myCSV10.csv,myCSV1.csv,myCSV11.csv'], ['Michael', 'myCSV1.csv'], ['Scott', 'myCSV1.csv,myCSV11.csv'], ['Steven', 'myCSV10.csv,myCSV11.csv']]\n",
      "MapReduce Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 21:58:27,877 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 - Async invoker 0 finished\n",
      "2022-01-04 21:58:27,877 [DEBUG] lithops.invokers -- ExecutorID ee8043-71 - Async invoker 1 finished\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceServerlessEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, params={'column':0})\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(obj, extra_args):\n",
    "    keys = []\n",
    "    # reading file\n",
    "    data = obj.data_stream.read().decode(encoding='utf-8')\n",
    "    # reading lines and extracting keys from data to list\n",
    "    for line in data.splitlines():\n",
    "        keys.append(line.split(',')[extra_args['column']])\n",
    "    # returning list of tuples (key_value, document_name)\n",
    "    return list(zip(keys[1:], [obj.key]*len(keys[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(documents):\n",
    "\n",
    "    # extracting key and value\n",
    "    res_list = [documents[0]]    \n",
    "    value = documents[1]\n",
    "    \n",
    "    # split value by ',' to get list of documents names and remove duplicates using set\n",
    "    docs_set = set(value.split(','))\n",
    "    # creating new \"value\" string from the set with no duplicates\n",
    "    docs_str = ','.join(docs_set)\n",
    "    \n",
    "    #appending value to res list \n",
    "    res_list.append(docs_str)\n",
    "    \n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "## Shuffle\n",
    "\n",
    "MapReduceServerlessEngine deploys both map and reduce tasks as serverless invocations.   \n",
    "However, once map stage completed, the result are transferred from the map tasks to the SQLite database located on the client machine (laptop in your case), then performed local shuffle and then invoked reduce tasks passing them relevant parameters.\n",
    "\n",
    "(To support your answers, feel free to use examples, Images, etc.)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Explain why this approach is not efficient and what are cons and pros of such architecture in general. In broader scope you may assume that MapReduceServerlessEngine executed in some powerful machine and not just laptop.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are talking about Big Data, if there were no Big Data, no need for serverless or map_reduce, we could do it all on our machine in a short time and also not losing time for moving data via network at all (serverless + shuffle).\n",
    "In case of Big Data, if we perfom shuffle locally we need to move all the results via network to our local machine, so this is the main case, it takes time depending how far it should be moved. Also as we are talking about Big Data, that means that \"even the strongest machine on planet can't process it\" (from Gil lections). So here we run small example, but if we talk about Big Data, local shuffle could be impossible. But let's assume it's not so Big, so as I mentioned before, we have problem moving all data via network to local machine. Secondly, even with strong machine, we want to do it effectivly and processing it all one one machine will be longer than processing it in parallel on different machines. On the other hand, while we do it locally on one machine we will not face struglers (case where most of processing done and we are waiting for struglers to continue processing all the data) and not facing rerun of maps or waiting till all shuffle done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**2. Suggest how can you improve shuffle so intermediate data will not be downloaded to the client at all and shuffle performed in the cloud as well. Explain pros and cons of the approaches you suggest.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write our remote function logic to store results in object storage then reducers will read from there or pass the results by key to other machines. It's up to us how we can implement it. So we can laverage location of machines, like region and reduce network moving, so if our map functions running for example in Germany, we can save our results in the same region and config reducers in Germany to process it. This way we minimizing shuffle phase by minimzing data transformation. Anyway, we still need to face implementing that logic for shuffling the data, lots of complicated data transfering and strugglers that can cause delays for all processing to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**3. Can you make serverless shuffle?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, it's seems that my answer for question 2 contains the answer for this question also. Anyway, yes, we can do it, for example we can run map functions and save temporary files to cloud object storage, also write logic for cloud functions that will be triggered and will read all that temporary files from object storage that generated by maps and sort them to provide each reducer a specific key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
