{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12f86a7",
   "metadata": {},
   "source": [
    "### Student:\n",
    "2. Alexander Gorelik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a2b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d9546",
   "metadata": {},
   "source": [
    "### Create local CSV file “mydata.csv” with 1000000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1181216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'mydata.csv'\n",
    "\n",
    "header = ['id', 'fruit', 'price', 'color']\n",
    "fruits = ['Orange', 'Grape', 'Apple', 'Banana', 'Pineapple', 'Avocado']\n",
    "colors = ['Red', 'Green', 'Yellow', 'Blue']\n",
    "\n",
    "with open(csv_file, 'w', newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for line_num in range(1, 1000000):\n",
    "        line = [f'{line_num}', f'{random.choice(fruits)}', random.randint(10, 100), f'{random.choice(colors)}']\n",
    "        csv_writer.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62116363",
   "metadata": {},
   "source": [
    "----\n",
    "# Task 1: CSV and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f9d81",
   "metadata": {},
   "source": [
    "### 1.1 & 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f06454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('mydata.csv')\n",
    "table_name = 'mydata'\n",
    "db_name = 'mydb.db'\n",
    "conn = None\n",
    "try:\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    df_data.to_sql(name=table_name, con=conn, if_exists='replace', index=False)\n",
    "except Error as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e89fa",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "----\n",
    "**SELECT** part is a **Project operation** ( operation on columns, selecting columns ) <br>\n",
    "**WHERE** part is a **Predicate operation** ( operation on rows, selecting rows )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8840cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_1 = \"SELECT fruit, price \" \\\n",
    "            \"FROM mydata \" \\\n",
    "            \"WHERE fruit = 'Banana' \"\n",
    "\n",
    "command_2 = \"SELECT fruit, color, price \" \\\n",
    "            \"FROM mydata \" \\\n",
    "            \"WHERE fruit = 'Avocado' AND color = 'Yellow'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = None\n",
    "try:\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cur = conn.cursor()\n",
    "    for row in cur.execute(command_2):\n",
    "        #print(row)\n",
    "        pass\n",
    "except Error as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b527fc",
   "metadata": {},
   "source": [
    "-----\n",
    "# Task 2: CSV and Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf719e",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_num = 0\n",
    "with open(csv_file) as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    for row in csv_reader:\n",
    "        lines_num += 1\n",
    "print(f'Number of lines in mydata.csv is {lines_num}')\n",
    "\n",
    "if not os.path.isdir('./parquets'):\n",
    "    os.mkdir('./parquets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c761cfa",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cafef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pv.read_csv(csv_file)\n",
    "pq.write_table(table, './parquets/mydatapyarrow.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e9562",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(csv_file)\n",
    "df.to_parquet('./parquets/mydatadask.parquet', write_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee48985",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ff4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_parquet('./parquets/mydatapandas.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1715c",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "Dask generate Parquet file differently as dask is a library for parallel computing and working with large datasets.\n",
    "Dask is a parallel computing framework that makes it easy to convert a lot of CSV files to Parquet files with a\n",
    "single operation.\n",
    "A Dask DataFrame contains multiple Pandas DataFrames.\n",
    "Each Pandas DataFrame is referred to a partition of the Dask DataFrame.\n",
    "Each partition in the Dask DataFrame is written out to disk in the Parquet file format.\n",
    "Dask writes out files in parallel so the Parquet files are written simultaneously.\n",
    "In our case, the Dask DataFrame consisted of one Pandas DataFrames as we have only one CSV file and as a result\n",
    "mydatadask.parquet directory has only one part.0.parquet\n",
    "This directory have _common_metadata that the schema of the dataset can be read from\n",
    "and _metadata that the to_parquet function generated that provides things that also can be got from each\n",
    "file's footer and scanning all files metadata footers can be painful, especially when we have millions of files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750dcce",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5984a2",
   "metadata": {},
   "source": [
    "# Task 3: Split CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9db2a",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7921da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size = os.path.getsize(csv_file)\n",
    "middle = int(file_size/2)\n",
    "\n",
    "print(f'mydata.csv file size is {file_size} bytes')\n",
    "print(f'middle is {middle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f2e50",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76919b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_chunk(file_csv, file_mid):\n",
    "    lines_count = 0\n",
    "    with open(file_csv, 'rb') as f:\n",
    "        d = f.read(file_mid).decode(encoding='utf-8')\n",
    "        lines_count = len(d.split('\\n'))\n",
    "    return lines_count\n",
    "\n",
    "\n",
    "def last_chunk(file_csv, file_mid):\n",
    "    lines_count = 0\n",
    "    columns_count = 0\n",
    "    with open(file_csv, 'rb') as f:\n",
    "        f.seek(file_mid, 0)\n",
    "        d = f.read().decode(encoding='utf-8')\n",
    "        lines_count = len(d.split('\\n'))\n",
    "    return lines_count-1   # subtracting 1 for automatically generated empty line at the end while creating a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02723d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = first_chunk('mydata.csv', middle)\n",
    "print(f'number of lines in the first chunk is {l1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = last_chunk('mydata.csv', middle)\n",
    "print(f'number of lines in the last chunk is {l2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of lines of 2 chunks is {l1+l2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca6ae2",
   "metadata": {},
   "source": [
    "### 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d671a",
   "metadata": {},
   "source": [
    "We are getting 1 line more, the reason is when we defining the middle, we split file in to two parts and the\n",
    "middle is very unlikely will fall exactly at the end of the line. So as a result it \"cuts\" one line in to two\n",
    "parts, then we have one part of the line as a line in the first chunk and the second part of a line as a row\n",
    "in the last chunk. That results in one additional line when we count total lines of two chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b36e44",
   "metadata": {},
   "source": [
    "### 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d05acc",
   "metadata": {},
   "source": [
    "In this algorithm we are processing csv file in chunks, the general idea that we reading the chunk, then we check if the last line of chunk is broken, so if yes, we continue reading the line byte by byte until we are reaching new line character so we will have complete rows and also counting these byte steps. Then we count the lines.<br>\n",
    "When we are getting to the next chunk we use our saved byte steps number we made in previous chunk to ignore these bytes in the new chunk, so we are actually have our rows starting from complete row.<br>\n",
    "*(We know that we could also use seek() to locate the pointer to ignore when we reading the bytes that we already read at the end of the previous chunk, but we thought that we want to leave seek() like an idea of processing file by chunks with defined size, so using seek again was like splitting the file in different size, so we sticked to our idea of ignore not relevant bytes after reading.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_lines_num(file_csv, chunk_size):\n",
    "    total_lines_number = 0\n",
    "    ignore_bytes = 0\n",
    "    csv_size = os.path.getsize(file_csv)\n",
    "    number_of_chunks = math.ceil(csv_size/chunk_size)\n",
    "    threshold = chunk_size\n",
    "    with open(file_csv, 'rb') as f:\n",
    "        for i in range(0, number_of_chunks):\n",
    "            f.seek(chunk_size*i, 0)\n",
    "            d = f.read(chunk_size).decode(encoding='utf-8')\n",
    "            relevant = d[ignore_bytes:]\n",
    "            if relevant == '':\n",
    "                break\n",
    "            ignore_bytes = 0\n",
    "            if relevant[-1] != '\\n':\n",
    "                while ignore_bytes <= threshold:\n",
    "                    ignore_bytes += 1\n",
    "                    ch = f.read(1).decode(encoding='utf-8')\n",
    "                    relevant += ch\n",
    "                    if ch == '\\n':\n",
    "                        break\n",
    "            lines_count = len(relevant.split('\\n'))-1  # since when splitting by \\n we get additional '' at the end\n",
    "            total_lines_number += lines_count\n",
    "    return total_lines_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_of_lines = calculate_total_lines_num('mydata.csv', middle)\n",
    "print(f'Total number of rows for two chunks is: {total_n_of_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5686c21",
   "metadata": {},
   "source": [
    "### 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad04d477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows after processing file by 16MB chunks is: 1000000\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 11\n",
    "total_number_of_lines = calculate_total_lines_num('mydata.csv', chunk_size*(1024**2))\n",
    "print(f'Total number of rows after processing file by 16MB chunks is: {total_number_of_lines}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a0bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
